{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "96f999f1",
      "metadata": {},
      "source": [
        "For Portfolio Dashboard Statistic cards, Speedometer gauges and Debrief Room display\n",
        "\n",
        "This notebook:\n",
        "A concise pipeline that parses each trade’s `ID` to its underlying, fetches daily OHLC via yfinance, and computes a running **S\\$** portfolio equity from **realised cashflows** (`Net Amount (S$) (Fuel)`), **ignoring `open` rows**; it outputs a CSV augmented with OHLC, date used, Yahoo symbol, and equity, with a **configurable starting capital** for portfolio analysis by LLM.\n",
        "\n",
        "1. Loads your **Trades History.csv** from C:\\Temp\\Dashboard\\data\\\n",
        "2. Add OHLC and Equity columns\n",
        "3. Exports CSV outputs to C:\\Temp\\Dashboard\\data\\f1_dashboard_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "17591427",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\boonm\\AppData\\Local\\Temp\\ipykernel_8816\\228389248.py:238: UserWarning: Parsing dates in %d %m %Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  trades[\"Date\"] = pd.to_datetime(trades[\"Date\"], errors=\"coerce\").dt.normalize()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ID → Underlying → Yahoo Symbol (sample) ===\n",
            "NVDA 251219C00130000           -> NVDA             -> NVDA\n",
            "DECK                           -> DECK             -> DECK\n",
            "TSLA                           -> TSLA             -> TSLA\n",
            "PLTR                           -> PLTR             -> PLTR\n",
            "PLTR                           -> PLTR             -> PLTR\n",
            "PLTR                           -> PLTR             -> PLTR\n",
            "PLTR                           -> PLTR             -> PLTR\n",
            "PLTR                           -> PLTR             -> PLTR\n",
            "PLTR                           -> PLTR             -> PLTR\n",
            "META                           -> META             -> META\n",
            "\n",
            "Diagnostics:\n",
            "  Closed rows used for equity: 100\n",
            "  Open rows ignored:           21\n",
            "  Sum of used cashflows (S$):  3,817.15\n",
            "Starting capital (S$):         10,000.00\n",
            "\n",
            "Done. Wrote: C:\\Temp\\Dashboard\\data\\f1_dashboard_outputs\\Trades History_with_OHLC.csv\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Augments a trades history CSV with daily OHLC (from ID→underlying),\n",
        "and computes PORTFOLIO_EQUITY using ONLY the 'Net Amount (S$) (Fuel)' column,\n",
        "ignoring rows where Status == 'open' (case/whitespace insensitive).\n",
        "\n",
        "Robust parsing:\n",
        "- Status: .strip().lower() == 'open' → ignored in equity\n",
        "- Net Amount (S$) (Fuel): handles parentheses negatives, commas, stray symbols\n",
        "\n",
        "Equity (in S$):\n",
        "  PORTFOLIO_EQUITY = STARTING_CAPITAL + cumulative sum of parsed net cashflows (closed rows only)\n",
        "Sorted by Date then original row order for stability.\n",
        "\n",
        "Also includes:\n",
        "- Robust ID parsing (equities, index options, OCC-like forms)\n",
        "- yfinance MultiIndex flattening → simple OHLC fields\n",
        "- numeric coercion for OHLC\n",
        "- Column renames you requested\n",
        "\n",
        "Author-configurable:\n",
        "- STARTING_CAPITAL\n",
        "- output folder\n",
        "- fill method for price date selection\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import typing as T\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =========================\n",
        "# User configuration\n",
        "# =========================\n",
        "TRADES_PATH = r\"C:\\Temp\\Dashboard\\data\\Trades History.csv\"   # adjust if needed\n",
        "\n",
        "# Custom output folder on Windows (use raw string to avoid escape issues)\n",
        "OUT_DIR = Path(r\"C:\\Temp\\Dashboard\\data\\f1_dashboard_outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_PATH = OUT_DIR / \"Trades History_with_OHLC.csv\"\n",
        "\n",
        "# Price-date selection for OHLC:\n",
        "# 'previous' -> last trading day <= trade date\n",
        "# 'nearest'  -> nearest trading day to trade date (before/after)\n",
        "FILL_METHOD = \"previous\"\n",
        "LEFT_BUFFER_DAYS  = 10\n",
        "RIGHT_BUFFER_DAYS = 5\n",
        "\n",
        "# Portfolio equity settings (in S$ to match cashflow column)\n",
        "STARTING_CAPITAL  = 10_000.0     # <-- change here if desired\n",
        "\n",
        "# Column with net cashflow in S$\n",
        "NET_CASH_COL = \"Net Amount (S$) (Fuel)\"\n",
        "\n",
        "# =========================\n",
        "# Index mapping\n",
        "# =========================\n",
        "INDEX_UNDERLYING_MAP = {\n",
        "    \"SPX\": \"^GSPC\",\n",
        "    \"SPXW\": \"^GSPC\",  # weeklies map to SPX\n",
        "    # Extend if needed: \"NDX\": \"^NDX\", \"RUT\": \"^RUT\"\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# Helpers\n",
        "# =========================\n",
        "def derive_underlying_from_id(id_str: str) -> str:\n",
        "    \"\"\"Extract logical underlying from ID (plain tickers, equity/index options, OCC-like).\"\"\"\n",
        "    if not id_str or not isinstance(id_str, str):\n",
        "        return \"\"\n",
        "    s = id_str.strip().upper()\n",
        "    base = s.split()[0]\n",
        "    # OCC-like no-space: NVDA251219C00130000\n",
        "    if re.match(r\"^[A-Z]+[0-9]{6}[CP][0-9A-Za-z]+$\", base):\n",
        "        m = re.match(r\"^([A-Z]+)[0-9]{6}[CP].+$\", base)\n",
        "        if m:\n",
        "            base = m.group(1)\n",
        "    if base == \"SPXW\":\n",
        "        base = \"SPX\"\n",
        "    return base\n",
        "\n",
        "def normalize_to_yahoo(symbol: str) -> str:\n",
        "    \"\"\"Normalize logical underlying to Yahoo-compatible ticker.\"\"\"\n",
        "    if not symbol:\n",
        "        return \"\"\n",
        "    s = symbol.strip().upper()\n",
        "    if s in INDEX_UNDERLYING_MAP:\n",
        "        return INDEX_UNDERLYING_MAP[s]\n",
        "    if s in {\"BTCUSD\", \"BTC-USD\"}:\n",
        "        return \"BTC-USD\"\n",
        "    if s in {\"ETHUSD\", \"ETH-USD\"}:\n",
        "        return \"ETH-USD\"\n",
        "    if \".\" in s:  # BRK.B -> BRK-B\n",
        "        left, right = s.split(\".\", 1)\n",
        "        if len(right) <= 3:\n",
        "            return f\"{left}-{right}\"\n",
        "    return s\n",
        "\n",
        "def _flatten_yf_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Flatten yfinance MultiIndex columns to simple OHLC fields.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"])\n",
        "    df = df.copy()\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        if len(df.columns.levels[0]) == 1 and \"Open\" not in df.columns.levels[0]:\n",
        "            df.columns = df.columns.droplevel(0)\n",
        "        elif len(df.columns.levels[1]) == 1 and \"Open\" not in df.columns.levels[1]:\n",
        "            df.columns = df.columns.droplevel(1)\n",
        "        else:\n",
        "            for lvl in [0, 1]:\n",
        "                if set([\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]).issubset(\n",
        "                    set(df.columns.get_level_values(lvl))\n",
        "                ):\n",
        "                    other = 1 - lvl\n",
        "                    if len(df.columns.levels[other]) == 1:\n",
        "                        df.columns = df.columns.droplevel(other)\n",
        "                    break\n",
        "    keep = [c for c in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"] if c in df.columns]\n",
        "    if not keep:\n",
        "        cols_lower = {c.lower(): c for c in df.columns}\n",
        "        mapped = {k: cols_lower.get(k.lower()) for k in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]}\n",
        "        keep = [mapped[k] for k in [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"] if mapped.get(k)]\n",
        "    df = df[keep].sort_index()\n",
        "    return df\n",
        "\n",
        "def _to_float(x):\n",
        "    \"\"\"Return float from scalar or 1-element Series; else NaN.\"\"\"\n",
        "    if isinstance(x, pd.Series):\n",
        "        x = x.dropna()\n",
        "        if not x.empty:\n",
        "            return float(x.iloc[0])\n",
        "        return np.nan\n",
        "    try:\n",
        "        return float(x)\n",
        "    except (TypeError, ValueError):\n",
        "        return np.nan\n",
        "\n",
        "def _parse_net_amount_sgd(series: pd.Series) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Parse 'Net Amount (S$) (Fuel)' robustly:\n",
        "      - Trim spaces\n",
        "      - Parentheses negatives: (1,234.56) -> -1234.56\n",
        "      - Remove thousands separators, currency letters/symbols\n",
        "    Returns float numpy array.\n",
        "    \"\"\"\n",
        "    s = series.astype(str).str.strip()\n",
        "    # (123.45) -> -123.45\n",
        "    s = s.str.replace(r\"^\\((.*)\\)$\", r\"-\\1\", regex=True)\n",
        "    # Remove commas\n",
        "    s = s.str.replace(\",\", \"\", regex=False)\n",
        "    # Remove everything except digits, dot, minus\n",
        "    s = s.str.replace(r\"[^\\d\\.\\-]+\", \"\", regex=True)\n",
        "    vals = pd.to_numeric(s, errors=\"coerce\").fillna(0.0).values\n",
        "    return vals\n",
        "\n",
        "# =========================\n",
        "# Data source: yfinance\n",
        "# =========================\n",
        "def fetch_ohlc_yf(symbols: T.List[str], start: datetime, end: datetime) -> dict:\n",
        "    try:\n",
        "        import yfinance as yf\n",
        "    except ImportError:\n",
        "        raise SystemExit(\"Please install yfinance: pip install yfinance --upgrade\")\n",
        "    out = {}\n",
        "    for ysym in symbols:\n",
        "        try:\n",
        "            df = yf.download(\n",
        "                ysym,\n",
        "                start=start.strftime(\"%Y-%m-%d\"),\n",
        "                end=(end + timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
        "                progress=False,\n",
        "                auto_adjust=False,\n",
        "                group_by=\"column\",\n",
        "                threads=True,\n",
        "            )\n",
        "            out[ysym] = _flatten_yf_df(df)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to fetch {ysym}: {e}\", file=sys.stderr)\n",
        "            out[ysym] = pd.DataFrame(columns=[\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"])\n",
        "        time.sleep(0.2)\n",
        "    return out\n",
        "\n",
        "# =========================\n",
        "# Locator for trade-day OHLC\n",
        "# =========================\n",
        "def pick_trade_day_price(df: pd.DataFrame, trade_date: pd.Timestamp, method: str):\n",
        "    \"\"\"Return numeric (O,H,L,C,AdjC, used_date) for the chosen trading day.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return (np.nan, np.nan, np.nan, np.nan, np.nan, pd.NaT)\n",
        "    df = df.sort_index()\n",
        "    if trade_date in df.index:\n",
        "        row = df.loc[trade_date]\n",
        "        return (_to_float(row.get(\"Open\")),\n",
        "                _to_float(row.get(\"High\")),\n",
        "                _to_float(row.get(\"Low\")),\n",
        "                _to_float(row.get(\"Close\")),\n",
        "                _to_float(row.get(\"Adj Close\")),\n",
        "                trade_date.normalize())\n",
        "    if method == \"previous\":\n",
        "        idx = df.index.searchsorted(trade_date, side=\"right\") - 1\n",
        "        if idx >= 0:\n",
        "            used = df.index[idx]; row = df.iloc[idx]\n",
        "            return (_to_float(row.get(\"Open\")), _to_float(row.get(\"High\")),\n",
        "                    _to_float(row.get(\"Low\")),  _to_float(row.get(\"Close\")),\n",
        "                    _to_float(row.get(\"Adj Close\")), used.normalize())\n",
        "        idx_after = df.index.searchsorted(trade_date, side=\"left\")\n",
        "        if idx_after < len(df):\n",
        "            used = df.index[idx_after]; row = df.iloc[idx_after]\n",
        "            return (_to_float(row.get(\"Open\")), _to_float(row.get(\"High\")),\n",
        "                    _to_float(row.get(\"Low\")),  _to_float(row.get(\"Close\")),\n",
        "                    _to_float(row.get(\"Adj Close\")), used.normalize())\n",
        "        return (np.nan, np.nan, np.nan, np.nan, np.nan, pd.NaT)\n",
        "    elif method == \"nearest\":\n",
        "        idx_right = df.index.searchsorted(trade_date, side=\"left\")\n",
        "        cands = []\n",
        "        if idx_right < len(df): cands.append(df.index[idx_right])\n",
        "        if idx_right > 0:       cands.append(df.index[idx_right-1])\n",
        "        if not cands:\n",
        "            return (np.nan, np.nan, np.nan, np.nan, np.nan, pd.NaT)\n",
        "        used = min(cands, key=lambda d: abs(d - trade_date))\n",
        "        row = df.loc[used]\n",
        "        return (_to_float(row.get(\"Open\")), _to_float(row.get(\"High\")),\n",
        "                _to_float(row.get(\"Low\")),  _to_float(row.get(\"Close\")),\n",
        "                _to_float(row.get(\"Adj Close\")), used.normalize())\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown FILL_METHOD: {method}\")\n",
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "def main():\n",
        "    trades = pd.read_csv(TRADES_PATH)\n",
        "    trades[\"Date\"] = pd.to_datetime(trades[\"Date\"], errors=\"coerce\").dt.normalize()\n",
        "\n",
        "    # Verify cashflow column exists\n",
        "    if NET_CASH_COL not in trades.columns:\n",
        "        raise ValueError(f\"Required column not found: '{NET_CASH_COL}'\")\n",
        "\n",
        "    # === Underlying and Yahoo symbol ===\n",
        "    trades[\"UNDERLYING_RAW\"] = trades[\"ID\"].astype(str).map(derive_underlying_from_id)\n",
        "    trades[\"YF_SYMBOL\"] = trades[\"UNDERLYING_RAW\"].map(normalize_to_yahoo)\n",
        "\n",
        "    # Log sample mappings\n",
        "    print(\"\\n=== ID → Underlying → Yahoo Symbol (sample) ===\")\n",
        "    for _, row in trades.head(10).iterrows():\n",
        "        print(f\"{row['ID']:30s} -> {row['UNDERLYING_RAW']:16s} -> {row['YF_SYMBOL']}\")\n",
        "\n",
        "    # === Fetch OHLC with buffer window ===\n",
        "    min_dt = trades[\"Date\"].min() - timedelta(days=LEFT_BUFFER_DAYS)\n",
        "    max_dt = trades[\"Date\"].max() + timedelta(days=RIGHT_BUFFER_DAYS)\n",
        "\n",
        "    unique_syms = sorted(s for s in trades[\"YF_SYMBOL\"].dropna().unique().tolist() if s)\n",
        "    ohlc_map = fetch_ohlc_yf(unique_syms, start=min_dt, end=max_dt)\n",
        "\n",
        "    # === Attach OHLC ===\n",
        "    used_dates, o_list, h_list, l_list, c_list, ac_list = [], [], [], [], [], []\n",
        "    for _, r in trades.iterrows():\n",
        "        ysym = r[\"YF_SYMBOL\"]; tdate = r[\"Date\"]\n",
        "        df = ohlc_map.get(ysym)\n",
        "        O,H,L,C,AC,used_dt = pick_trade_day_price(df, pd.Timestamp(tdate), FILL_METHOD)\n",
        "        used_dates.append(used_dt)\n",
        "        o_list.append(O); h_list.append(H); l_list.append(L); c_list.append(C); ac_list.append(AC)\n",
        "\n",
        "    trades[\"PX_DATE_USED\"]  = pd.to_datetime(used_dates)\n",
        "    trades[\"PX_OPEN\"]       = o_list\n",
        "    trades[\"PX_HIGH\"]       = h_list\n",
        "    trades[\"PX_LOW\"]        = l_list\n",
        "    trades[\"PX_CLOSE\"]      = c_list\n",
        "    trades[\"PX_ADJ_CLOSE\"]  = ac_list\n",
        "\n",
        "    # === Portfolio equity using 'Net Amount (S$) (Fuel)' ===\n",
        "    cashflow_sgd_all = _parse_net_amount_sgd(trades[NET_CASH_COL])\n",
        "\n",
        "    # Ignore 'open' rows (strip + lower)\n",
        "    if \"Status\" in trades.columns:\n",
        "        status_norm = trades[\"Status\"].astype(str).str.strip().str.lower()\n",
        "        closed_mask = status_norm != \"open\"\n",
        "    else:\n",
        "        closed_mask = pd.Series(True, index=trades.index)\n",
        "\n",
        "    cashflow_used = np.where(closed_mask.values, cashflow_sgd_all, 0.0)\n",
        "\n",
        "    # Order by Date then original order to compute cumulative equity\n",
        "    trades[\"_ROW_\"] = np.arange(len(trades))\n",
        "    sort_idx = np.lexsort((trades[\"_ROW_\"].values, trades[\"Date\"].values.astype(\"datetime64[ns]\")))\n",
        "    equity_sorted = STARTING_CAPITAL + np.cumsum(cashflow_used[sort_idx])\n",
        "\n",
        "    inv_idx = np.empty_like(sort_idx)\n",
        "    inv_idx[sort_idx] = np.arange(len(sort_idx))\n",
        "    trades[\"PORTFOLIO_EQUITY\"] = equity_sorted[inv_idx].astype(float)\n",
        "    trades.drop(columns=[\"_ROW_\"], inplace=True)\n",
        "\n",
        "    # === Diagnostics ===\n",
        "    n_open_ignored = int((~closed_mask).sum())\n",
        "    total_used = float(np.nansum(cashflow_used))\n",
        "    print(f\"\\nDiagnostics:\")\n",
        "    print(f\"  Closed rows used for equity: {int(closed_mask.sum())}\")\n",
        "    print(f\"  Open rows ignored:           {n_open_ignored}\")\n",
        "    print(f\"  Sum of used cashflows (S$):  {total_used:,.2f}\")\n",
        "    if np.allclose(cashflow_used, 0.0):\n",
        "        # Help the user spot parsing issues quickly\n",
        "        print(\"\\nWARNING: All parsed cashflows are 0. Verify formatting of 'Net Amount (S$) (Fuel)'.\")\n",
        "        print(\"Sample raw values:\")\n",
        "        print(trades[NET_CASH_COL].head(10).to_list())\n",
        "\n",
        "    print(f\"Starting capital (S$):         {STARTING_CAPITAL:,.2f}\")\n",
        "\n",
        "    # === Rename columns per your request ===\n",
        "    rename_map = {\n",
        "        \"UNDERLYING_RAW\": \"UNDERLYING_TICKER\",\n",
        "        \"PX_DATE_USED\":   \"DATE_USED\",\n",
        "        \"PX_OPEN\":        \"OPEN\",\n",
        "        \"PX_HIGH\":        \"HIGH\",\n",
        "        \"PX_LOW\":         \"LOW\",\n",
        "        \"PX_CLOSE\":       \"CLOSE\",\n",
        "        \"PX_ADJ_CLOSE\":   \"ADJ_CLOSE\",\n",
        "    }\n",
        "    trades = trades.rename(columns=rename_map)\n",
        "\n",
        "    trades.to_csv(OUTPUT_PATH, index=False)\n",
        "    print(f\"\\nDone. Wrote: {OUTPUT_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "164ae338",
      "metadata": {},
      "source": [
        "For Portfolio Dashboard F1 Grand Prix Circuit Chart and Championship Standings display\n",
        "\n",
        "This notebook:\n",
        "1. Loads your **Trades History.csv** from C:\\Temp\\Dashboard\\data\\\n",
        "2. Computes overall performance metrics \n",
        "3. Computes points based on F1 points system and performance metric per Strategy\n",
        "5. Exports CSV outputs to C:\\Temp\\Dashboard\\data\\f1_dashboard_outputs\n",
        "\n",
        "**Instructions:** Place `Trades History.csv` in the same folder as this notebook. Then run cells top-to-bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "87dd6a54",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[DEBUG] equity_daily has 40 points (2024-12-19 → 2025-09-23)\n",
            "[DEBUG] Wrote portfolio_metrics.csv\n",
            "\n",
            "[DEBUG] --- Strategy breakdown (Sharpe/CAGR/ROI/TE, anchored to start cap) ---\n",
            "[Assignment] Trades=1 | ROI=0.005496 | CAGR=nan | Sharpe(Trade)=nan | MDD=0.000000\n",
            "[Bear Call Spread] Trades=8 | ROI=0.027060 | CAGR=0.082515 | Sharpe(Trade)=3.905672 | MDD=0.000000\n",
            "[Bull Put Spread] Trades=15 | ROI=0.003694 | CAGR=0.006313 | Sharpe(Trade)=0.021952 | MDD=-0.046222\n",
            "[Iron Condor] Trades=2 | ROI=0.052028 | CAGR=1.798792 | Sharpe(Trade)=0.805414 | MDD=0.000000\n",
            "[Long Call] Trades=1 | ROI=0.196509 | CAGR=nan | Sharpe(Trade)=nan | MDD=0.000000\n",
            "[Short Call] Trades=5 | ROI=0.015830 | CAGR=0.046962 | Sharpe(Trade)=2.100745 | MDD=0.000000\n",
            "[Short Put] Trades=10 | ROI=0.074236 | CAGR=0.145941 | Sharpe(Trade)=0.701309 | MDD=-0.003251\n",
            "[Short Strangle] Trades=3 | ROI=0.006862 | CAGR=0.027828 | Sharpe(Trade)=0.153307 | MDD=-0.014833\n",
            "\n",
            "Saved outputs to: C:\\Temp\\Dashboard\\data\\f1_dashboard_outputs\n",
            " - trade_level_pnl.csv ✓\n",
            " - overall_equity_curve.csv ✓\n",
            " - portfolio_metrics.csv ✓\n",
            " - strategy_points.csv ✓\n"
          ]
        }
      ],
      "source": [
        "# ===================== F1 Dashboard: CSV Pipeline (Updated Points System) =====================\n",
        "# Portfolio (Telemetry): Date-Sharpe from observed closed-trade dates (non-annualised, rf=0, ddof=1)\n",
        "# Strategy (Podium):     Trade-Sharpe per round-trip (non-annualised, rf=0, ddof=1), CAGR annualised, ROI vs fixed start cap\n",
        "# Calmar removed. New points:\n",
        "#   Core (scaled): Sharpe 25, TE 18, CAGR 15\n",
        "#   Bonuses: ROI +4 (highest), WinRate +4 (highest), MDD +4 (lowest |MDD|)\n",
        "# -----------------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# -------------------------- Config --------------------------\n",
        "DAYFIRST                   = True\n",
        "PORTFOLIO_STARTING_CAPITAL = 10_000.0\n",
        "PER_STRATEGY_STARTING_CAP  = 10_000.0\n",
        "\n",
        "# Sharpe RF = 0 (non-annualised)\n",
        "RISK_FREE_RATE_MODE  = \"annualized\"   # \"daily\" or \"annualized\" (kept for completeness)\n",
        "RISK_FREE_RATE_VALUE = 0.00\n",
        "RF_DAILY = float(RISK_FREE_RATE_VALUE) if RISK_FREE_RATE_MODE.lower() == \"daily\" else float(RISK_FREE_RATE_VALUE)/252.0\n",
        "\n",
        "# ------------------ Points (UPDATED) ------------------\n",
        "# Core points (scaled to best performer; negatives clamp to 0)\n",
        "W_SHARPE = 25\n",
        "W_TE     = 18\n",
        "W_CAGR   = 15\n",
        "\n",
        "# Bonuses\n",
        "BONUS_ROI_MAX         = 4   # highest ROI\n",
        "BONUS_WINRATE_MAX     = 4   # highest WinRate\n",
        "BONUS_MDD_LOWEST_ABS  = 4   # lowest |MDD|\n",
        "\n",
        "# ------------------ Locate inputs -------------------\n",
        "CWD = Path.cwd()\n",
        "candidates = [\n",
        "    CWD / \"Trades History.csv\",\n",
        "    CWD.parent / \"Trades History.csv\",\n",
        "    CWD / \"data\" / \"Trades History.csv\",\n",
        "    CWD.parent / \"data\" / \"Trades History.csv\",\n",
        "    Path(\"C:/Temp/Dashboard/data/Trades History.csv\"),\n",
        "]\n",
        "TRADES_FILE = next((p for p in candidates if p.exists()), None)\n",
        "if TRADES_FILE is None:\n",
        "    raise FileNotFoundError(\"Could not find 'Trades History.csv'\")\n",
        "\n",
        "ROOT = TRADES_FILE.parent.parent if TRADES_FILE.parent.name.lower() == \"data\" else TRADES_FILE.parent\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "OUTDIR   = DATA_DIR / \"f1_dashboard_outputs\"\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ------------------------------ Helpers ---------------------------------------\n",
        "def parse_money_sgd(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    s = str(x).replace('S$','').replace('$','').replace(',','').strip()\n",
        "    s = s.replace('(', '-').replace(')', '')\n",
        "    try: return float(s)\n",
        "    except ValueError: return np.nan\n",
        "\n",
        "def ensure_columns(df, req):\n",
        "    miss = [c for c in req if c not in df.columns]\n",
        "    if miss: raise ValueError(f\"Missing required columns: {miss}\")\n",
        "\n",
        "def daily_sharpe(returns, rf_daily=0.0):\n",
        "    \"\"\"Sharpe using non-annualised returns and sample std (ddof=1).\"\"\"\n",
        "    r = pd.Series(returns).dropna()\n",
        "    if r.empty or r.std(ddof=1) == 0: return np.nan\n",
        "    excess = r - rf_daily\n",
        "    return float(excess.mean() / r.std(ddof=1))\n",
        "\n",
        "def max_drawdown_with_dates(equity_series):\n",
        "    \"\"\"Return (max_drawdown, peak_date, trough_date, recovery_date) on observed index.\"\"\"\n",
        "    s = pd.Series(equity_series).dropna().astype(float)\n",
        "    if s.empty: return np.nan, None, None, None\n",
        "    \n",
        "    # Ensure unique index by taking the last value for each date\n",
        "    s = s.groupby(s.index).last()\n",
        "    \n",
        "    roll_max = s.cummax()\n",
        "    dd = (s / roll_max) - 1.0\n",
        "    trough_date = dd.idxmin()\n",
        "    if pd.isna(trough_date): return np.nan, None, None, None\n",
        "    peak_date = s.loc[:trough_date].idxmax()\n",
        "    rec_slice = s.loc[trough_date:]\n",
        "    rec_mask = rec_slice >= s.loc[peak_date]\n",
        "    recovery_date = rec_mask[rec_mask].index.min() if rec_mask.any() else None\n",
        "    \n",
        "    # Ensure we return a scalar value\n",
        "    max_dd_value = dd.loc[trough_date]\n",
        "    if hasattr(max_dd_value, '__len__') and len(max_dd_value) > 1:\n",
        "        max_dd_value = max_dd_value.iloc[-1]  # Take the last value if still multiple\n",
        "    \n",
        "    return float(max_dd_value), peak_date, trough_date, recovery_date\n",
        "\n",
        "def annualize_from_equity(equity_series):\n",
        "    \"\"\"CAGR from equity series using calendar years between first/last observed dates.\"\"\"\n",
        "    s = pd.Series(equity_series).dropna().astype(float)\n",
        "    if len(s) < 2 or s.iloc[0] <= 0: return np.nan\n",
        "    years = (s.index[-1] - s.index[0]).days / 365.25\n",
        "    if years <= 0: return np.nan\n",
        "    return float((s.iloc[-1] / s.iloc[0])**(1/years) - 1.0)\n",
        "\n",
        "def clamp_nonneg(x): return np.maximum(np.asarray(x, dtype=float), 0.0)\n",
        "\n",
        "def scaled_points(series, weight):\n",
        "    \"\"\"Scale non-negative metric to [0, weight] by dividing by max.\"\"\"\n",
        "    s = pd.Series(clamp_nonneg(series))\n",
        "    m = s.max(skipna=True)\n",
        "    if not np.isfinite(m) or m <= 0:\n",
        "        return pd.Series(0.0, index=s.index)\n",
        "    return weight * (s / m)\n",
        "\n",
        "# ------------------------------ Load trades -----------------------------------\n",
        "tr_raw = pd.read_csv(TRADES_FILE, encoding=\"utf-8\")\n",
        "rename_map = {\n",
        "    \"Net Amount (S$) (Fuel)\": \"Net Amount (S$)\",\n",
        "    \"Strategy (Car)\": \"Strategy\",\n",
        "    \"Risk tolerance (Driver)\": \"Driver\",\n",
        "    \"Acct ID (Team)\": \"Team\",  # Added this mapping for the team column\n",
        "    \"Team\": \"Team\",\n",
        "}\n",
        "tr = tr_raw.rename(columns={k:v for k,v in rename_map.items() if k in tr_raw.columns})\n",
        "ensure_columns(tr, ['Date','Type','ID','Net Amount (S$)','Strategy','Status'])\n",
        "if 'Team' not in tr.columns: tr['Team'] = 'Default'\n",
        "\n",
        "tr['Date'] = pd.to_datetime(tr['Date'], dayfirst=DAYFIRST, errors='coerce')\n",
        "tr['NetAmount_SGD'] = tr['Net Amount (S$)'].apply(parse_money_sgd)\n",
        "\n",
        "is_closed = tr['Status'].astype(str).str.strip().str.lower().eq('closed')\n",
        "tr_closed = tr[is_closed].copy()\n",
        "tr_open   = tr[~is_closed].copy()\n",
        "\n",
        "# ---------------------- Round-trip trade-level ----------------------\n",
        "def has_both_sides(g):\n",
        "    sides = g['Type'].astype(str).str.strip().str.lower().unique()\n",
        "    return ('buy' in sides) and ('sell' in sides)\n",
        "\n",
        "rows = []\n",
        "for (iid, strat), g in tr_closed.groupby(['ID','Strategy'], dropna=False):\n",
        "    if has_both_sides(g):\n",
        "        # Get the team for this trade (should be consistent within the trade)\n",
        "        team = g['Team'].iloc[0] if not g['Team'].empty else 'Default'\n",
        "        rows.append({\n",
        "            'ID': iid,\n",
        "            'Strategy': strat,\n",
        "            'Team': team,  # Added team to trade level data\n",
        "            'CloseDate': g['Date'].max(),\n",
        "            'TradePnL_SGD': g['NetAmount_SGD'].sum(),\n",
        "            'Legs': len(g),\n",
        "        })\n",
        "trade_level = pd.DataFrame(rows).sort_values(['CloseDate','Strategy','ID'])\n",
        "trade_level.to_csv(OUTDIR / 'trade_level_pnl.csv', index=False)\n",
        "\n",
        "if trade_level.empty:\n",
        "    raise ValueError(\"No closed round-trip trades found.\")\n",
        "\n",
        "# ---------------------- Portfolio (Telemetry) ----------------------\n",
        "# Equity = starting capital + cumulative closed-leg PnL (observed closed-trade dates only)\n",
        "daily_closed_all = (\n",
        "    tr_closed.groupby('Date', as_index=True)['NetAmount_SGD']\n",
        "             .sum().sort_index()\n",
        ")\n",
        "equity_daily = (PORTFOLIO_STARTING_CAPITAL + daily_closed_all.cumsum()).rename('Equity')\n",
        "\n",
        "print(f\"\\n[DEBUG] equity_daily has {len(equity_daily)} points \"\n",
        "      f\"({equity_daily.index.min().date()} → {equity_daily.index.max().date()})\")\n",
        "\n",
        "# Portfolio returns on observed days; fix first return vs start cap; drop zero-PnL days\n",
        "raw_returns = (equity_daily / equity_daily.shift(1) - 1.0)\n",
        "first_date = equity_daily.index[0]\n",
        "raw_returns.loc[first_date] = (equity_daily.iloc[0] - PORTFOLIO_STARTING_CAPITAL) / PORTFOLIO_STARTING_CAPITAL\n",
        "nonzero_mask = daily_closed_all != 0\n",
        "daily_ret_port = raw_returns.where(nonzero_mask, np.nan).dropna()\n",
        "\n",
        "port_sharpe = daily_sharpe(daily_ret_port, rf_daily=0.0)\n",
        "\n",
        "# Month-end equity for charts only\n",
        "equity_monthly = equity_daily.resample('ME').last().dropna()\n",
        "overall_df = equity_monthly.reset_index().rename(columns={'Date':'Month'})\n",
        "overall_df.to_csv(OUTDIR / 'overall_equity_curve.csv', index=False)\n",
        "\n",
        "# Portfolio headline metrics (UNCHANGED - no team info here as requested)\n",
        "port_cagr = annualize_from_equity(equity_daily)\n",
        "port_mdd, _, _, _ = max_drawdown_with_dates(equity_daily)\n",
        "\n",
        "gp = trade_level['TradePnL_SGD'].clip(lower=0).sum()\n",
        "gl = -trade_level['TradePnL_SGD'].clip(upper=0).sum()\n",
        "port_pf = float(gp / gl) if gl > 0 else np.nan\n",
        "port_wr = float((trade_level['TradePnL_SGD'] > 0).mean()) if len(trade_level) else np.nan\n",
        "\n",
        "end_eq = float(equity_daily.iloc[-1])\n",
        "roi = (end_eq - PORTFOLIO_STARTING_CAPITAL) / PORTFOLIO_STARTING_CAPITAL\n",
        "\n",
        "portfolio_metrics = pd.DataFrame([{\n",
        "    'StartDate': equity_daily.index[0].date(),\n",
        "    'EndDate':   equity_daily.index[-1].date(),\n",
        "    'StartEquity': PORTFOLIO_STARTING_CAPITAL,\n",
        "    'EndEquity':   end_eq,\n",
        "    'CAGR': port_cagr,\n",
        "    'Sharpe': port_sharpe,          # Date-Sharpe (daily, ddof=1, rf=0)\n",
        "    'MaxDrawdown': port_mdd,        # scalar only\n",
        "    'WinRate': port_wr,\n",
        "    'ProfitFactor': port_pf,\n",
        "    'ROI': roi,                     # total (not annualised)\n",
        "    'OpenTrades': int(tr_open['ID'].nunique()),\n",
        "}])\n",
        "portfolio_metrics.to_csv(OUTDIR / 'portfolio_metrics.csv', index=False)\n",
        "print(\"[DEBUG] Wrote portfolio_metrics.csv\")\n",
        "\n",
        "# ---------------------- Strategy (Podium) ----------------------\n",
        "strategy_rows = []\n",
        "print(\"\\n[DEBUG] --- Strategy breakdown (Sharpe/CAGR/ROI/TE, anchored to start cap) ---\")\n",
        "\n",
        "# Create a mapping of strategy to team for later use\n",
        "strategy_team_map = trade_level.groupby('Strategy')['Team'].first().to_dict()\n",
        "\n",
        "for strat, tl_s in trade_level.groupby('Strategy', dropna=False):\n",
        "    tl_s = tl_s.sort_values('CloseDate').copy()\n",
        "\n",
        "    total_pnl = tl_s['TradePnL_SGD'].sum() if not tl_s.empty else 0.0\n",
        "    roi_s = total_pnl / PER_STRATEGY_STARTING_CAP\n",
        "\n",
        "    if not tl_s.empty:\n",
        "        close_dates = pd.to_datetime(tl_s['CloseDate'].values)\n",
        "        cum_pnl     = tl_s['TradePnL_SGD'].cumsum().values\n",
        "        eq_cum      = PER_STRATEGY_STARTING_CAP + cum_pnl\n",
        "\n",
        "        first_dt    = pd.to_datetime(tl_s['CloseDate'].min())\n",
        "        last_dt     = pd.to_datetime(tl_s['CloseDate'].max())\n",
        "\n",
        "        # equity incl. start-cap anchor (for MDD baseline)\n",
        "        # Handle potential duplicate dates by creating daily equity series\n",
        "        equity_data = []\n",
        "        equity_dates = []\n",
        "        \n",
        "        # Add starting capital\n",
        "        start_anchor_date = first_dt - pd.Timedelta(days=1)\n",
        "        equity_data.append(PER_STRATEGY_STARTING_CAP)\n",
        "        equity_dates.append(start_anchor_date)\n",
        "        \n",
        "        # Add cumulative equity for each trade close date\n",
        "        for i, date in enumerate(close_dates):\n",
        "            equity_data.append(eq_cum[i])\n",
        "            equity_dates.append(date)\n",
        "        \n",
        "        # Create series and handle duplicates by taking the last value for each date\n",
        "        eq_with_start = pd.Series(data=equity_data, index=equity_dates)\n",
        "        eq_with_start = eq_with_start.groupby(eq_with_start.index).last()\n",
        "\n",
        "        # Trade-Sharpe (include first jump from start cap)\n",
        "        first_ret  = (eq_cum[0] - PER_STRATEGY_STARTING_CAP) / PER_STRATEGY_STARTING_CAP\n",
        "        subsequent = pd.Series(eq_cum).pct_change().iloc[1:].replace([np.inf, -np.inf], np.nan)\n",
        "        ret_trade  = pd.concat([pd.Series([first_ret]), subsequent], ignore_index=True).dropna()\n",
        "        sharpe_s   = daily_sharpe(ret_trade, rf_daily=0.0) if len(ret_trade) >= 2 else np.nan\n",
        "\n",
        "        # CAGR (annualised) between first & last CLOSE (365.25)\n",
        "        days        = (last_dt - first_dt).days\n",
        "        years_36525 = days / 365.25 if days > 0 else np.nan\n",
        "        cagr_s      = ( (eq_cum[-1] / PER_STRATEGY_STARTING_CAP) ** (1/years_36525) - 1\n",
        "                       ) if pd.notna(years_36525) and years_36525 > 0 else np.nan\n",
        "\n",
        "        # MDD (for MDD bonus only)\n",
        "        mdd_s, _, _, _ = max_drawdown_with_dates(eq_with_start)\n",
        "    else:\n",
        "        first_dt = last_dt = None\n",
        "        sharpe_s = cagr_s = mdd_s = np.nan\n",
        "\n",
        "    print(f\"[{strat}] Trades={len(tl_s)} | ROI={roi_s:.6f} | \"\n",
        "          f\"CAGR={cagr_s if pd.notna(cagr_s) else np.nan:.6f} | \"\n",
        "          f\"Sharpe(Trade)={sharpe_s if pd.notna(sharpe_s) else np.nan:.6f} | \"\n",
        "          f\"MDD={mdd_s if pd.notna(mdd_s) else np.nan:.6f}\")\n",
        "\n",
        "    strategy_rows.append({\n",
        "        'Strategy': strat,\n",
        "        'Team': strategy_team_map.get(strat, 'Default'),  # Added team information\n",
        "        'CAGR': cagr_s,         # annualised, relative to PER_STRATEGY_STARTING_CAP\n",
        "        'Sharpe': sharpe_s,     # Trade-Sharpe (non-annualised, ddof=1, rf=0)\n",
        "        'MDD': mdd_s,           # for bonus only\n",
        "        'ROI': roi_s,           # for ROI bonus\n",
        "    })\n",
        "\n",
        "strategy_core = pd.DataFrame(strategy_rows)\n",
        "\n",
        "# Trade-level stats per Strategy (for TE/WinRate)\n",
        "trade_stats_strat = (\n",
        "    trade_level.groupby('Strategy')\n",
        "               .agg({\n",
        "                   'TradePnL_SGD': ['sum',\n",
        "                                    lambda s: (s > 0).sum(),\n",
        "                                    lambda s: (s <= 0).sum(),\n",
        "                                    'count',\n",
        "                                    lambda s: s[s > 0].mean(),\n",
        "                                    lambda s: -s[s <= 0].mean()]\n",
        "               })\n",
        ")\n",
        "trade_stats_strat.columns = ['TotalPnL','Wins','Losses','Trades','AvgWin','AvgLoss']\n",
        "trade_stats_strat = trade_stats_strat.fillna(0.0)\n",
        "trade_stats_strat['WinRate']    = trade_stats_strat['Wins'] / trade_stats_strat['Trades']\n",
        "trade_stats_strat['Expectancy'] = (trade_stats_strat['WinRate'] * trade_stats_strat['AvgWin']) - ((1 - trade_stats_strat['WinRate']) * trade_stats_strat['AvgLoss'])\n",
        "trade_stats_strat['TE']         = trade_stats_strat['Expectancy'] * trade_stats_strat['Trades']\n",
        "trade_stats_strat = trade_stats_strat.reset_index()\n",
        "\n",
        "# Merge & score (NEW points)\n",
        "strategy_points = strategy_core.merge(trade_stats_strat, on='Strategy', how='left')\n",
        "\n",
        "# Core points (scaled to best performer; negatives -> 0)\n",
        "def scaled(series, weight):\n",
        "    s = pd.Series(series).copy()\n",
        "    s = s.clip(lower=0)   # clamp negatives -> 0\n",
        "    mx = s.max(skipna=True)\n",
        "    return weight * (s / mx) if (pd.notna(mx) and mx > 0) else s*0\n",
        "\n",
        "strategy_points['Sharpe_points'] = scaled(strategy_points['Sharpe'], W_SHARPE).fillna(0.0)\n",
        "strategy_points['TE_points']     = scaled(strategy_points['TE'],     W_TE).fillna(0.0)\n",
        "strategy_points['CAGR_points']   = scaled(strategy_points['CAGR'],   W_CAGR).fillna(0.0)\n",
        "\n",
        "# Bonuses: ROI (highest), WinRate (highest), MDD (lowest |MDD|)\n",
        "strategy_points['ROI_Bonus']     = (strategy_points['ROI'] == strategy_points['ROI'].max()).astype(float) * BONUS_ROI_MAX\n",
        "strategy_points['WinRate_Bonus'] = (strategy_points['WinRate'] == strategy_points['WinRate'].max()).astype(float) * BONUS_WINRATE_MAX\n",
        "min_abs_mdd = strategy_points['MDD'].abs().min() if strategy_points['MDD'].notna().any() else np.nan\n",
        "strategy_points['MDD_Bonus']     = (strategy_points['MDD'].abs() == min_abs_mdd).astype(float) * BONUS_MDD_LOWEST_ABS\n",
        "strategy_points[['ROI_Bonus','WinRate_Bonus','MDD_Bonus']] = strategy_points[['ROI_Bonus','WinRate_Bonus','MDD_Bonus']].fillna(0.0)\n",
        "\n",
        "# Total points\n",
        "point_cols = ['Sharpe_points','TE_points','CAGR_points','ROI_Bonus','WinRate_Bonus','MDD_Bonus']\n",
        "strategy_points['TotalPoints'] = strategy_points[point_cols].sum(axis=1, skipna=True)\n",
        "strategy_points = strategy_points.sort_values('TotalPoints', ascending=False).reset_index(drop=True)\n",
        "strategy_points.insert(0, 'POS', range(1, len(strategy_points)+1))\n",
        "\n",
        "# Reorder columns to put Team right after Strategy\n",
        "cols = strategy_points.columns.tolist()\n",
        "# Find the index of 'Strategy' and 'Team'\n",
        "strategy_idx = cols.index('Strategy')\n",
        "team_idx = cols.index('Team')\n",
        "\n",
        "# Reorder to put Team right after Strategy\n",
        "if team_idx != strategy_idx + 1:\n",
        "    cols.pop(team_idx)  # Remove Team from its current position\n",
        "    cols.insert(strategy_idx + 1, 'Team')  # Insert Team right after Strategy\n",
        "    strategy_points = strategy_points[cols]\n",
        "\n",
        "strategy_points.to_csv(OUTDIR / 'strategy_points.csv', index=False)\n",
        "\n",
        "# ------------------------------ Summary --------------------------------------\n",
        "print(\"\\nSaved outputs to:\", OUTDIR.resolve())\n",
        "for f in [\n",
        "    'trade_level_pnl.csv',\n",
        "    'overall_equity_curve.csv',\n",
        "    'portfolio_metrics.csv',\n",
        "    'strategy_points.csv',\n",
        "]:\n",
        "    print(\" -\", f, \"✓\" if (OUTDIR / f).exists() else \"✗\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
